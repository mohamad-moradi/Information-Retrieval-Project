{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19937d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from gensim.models import KeyedVectors\n",
    "# # give a path of model to load function\n",
    "# model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "import gensim.downloader as api\n",
    "model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d71f4bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio.windows_events import NULL\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import difflib\n",
    "\n",
    "import pickle\n",
    "from os.path import exists\n",
    "\n",
    "import spacy\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"merge_entities\")\n",
    "\n",
    "stemm = RSLPStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "944f158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10d67a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_set = {}\n",
    "bm25 = NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e48ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('CISI/CISI.ALL') as f:\n",
    "        lines = \"\"\n",
    "        for l in f.readlines():\n",
    "            lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
    "        lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
    "doc_id = \"\"\n",
    "doc_text = \"\"\n",
    "for l in lines:\n",
    "    if l.startswith(\".I\"):\n",
    "        doc_id = int(l.split(\" \")[1].strip())\n",
    "    elif l.startswith(\".X\"):\n",
    "        doc_set[doc_id] = doc_text.lstrip(\" \")\n",
    "        doc_id = \"\"\n",
    "        doc_text = \"\"\n",
    "    else:\n",
    "        doc_text += l.strip()[3:] + \" \"  # The first 3 characters of a line can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "79d11024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    token_words = word_tokenize(text)\n",
    "    stem_sentence = []\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(ps.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    doc = nlp(\"\".join(stem_sentence))\n",
    "    words = []\n",
    "    for token in doc:\n",
    "        if not token.is_punct and not token.is_space and not token.is_stop:\n",
    "            tk = token.lemma_\n",
    "            words.append(tk)\n",
    "    return words\n",
    "\n",
    "\n",
    "## function for building inverted index using bm25\n",
    "\n",
    "def building_index(tokenized_corpus):\n",
    "    bm25_in = BM25Okapi(tokenized_corpus)\n",
    "    return bm25_in;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bfc530a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(doc_set.values())\n",
    "\n",
    "if not exists('BM25.index'):\n",
    "    tokenized_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "    # build index for Docs\n",
    "    bm25 = building_index(tokenized_corpus)\n",
    "    with open('BM25.index', 'wb') as outp:\n",
    "        pickle.dump(bm25, outp, pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open('BM25.index', 'rb') as inp:\n",
    "        bm25 = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6a3433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_from_query_new(query, bm25):\n",
    "    tokenized_query = preprocess_text(query)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    most_relevant_document = np.argsort(-scores)\n",
    "    return most_relevant_document[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed75f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello_world():\n",
    "    query = \"Describe presently working and planned systems for publishing and printing original papers by computer, and then saving the byproduct, articles coded in data-processing form, for further use in retrieval.\"\n",
    "    \n",
    "    most_relevant_documents = results_from_query_new(query, bm25)\n",
    "\n",
    "    return most_relevant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3aaae2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = hello_world()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c066b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _embed(tokens):\n",
    "    embedding = np.mean(\n",
    "        np.array([model[token] for token in tokens if token in model]),\n",
    "        axis=0,\n",
    "    )\n",
    "    unit_embedding = embedding / (embedding**2).sum()**0.5\n",
    "    return unit_embedding\n",
    "\n",
    "def rank(tokenized_query, tokenized_documents):\n",
    "    query_embedding = _embed(tokenized_query) # (E,)\n",
    "    document_embeddings = np.array([_embed(document) for document in tokenized_documents]) # (N, E)\n",
    "    scores = document_embeddings.dot(query_embedding)\n",
    "    index_rankings = np.argsort(scores)[::-1]\n",
    "    return index_rankings, np.sort(scores)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "30b3c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [preprocess_text(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e4b8feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_doc = [tokenized_corpus[i] for i in m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "035c6a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"    A new method is described to extract significant phrases in the title and the abstreact of scientific or technical documents.  The method is based upon a text structure analysis and uses a relatively small dictionary. The dictionary has been constructed based on the knowledge about concepts in the field of science or technology and some lexical knowledge.  For significant phrases and their component items may be used in different meanings among the fields.  A text analysius approach has been applied to select significant phrases as substantial and semantic information carriers of the contents of the abstract.      The results of the experiment for five sets of documents have shown that the significant phrases are effectively extracted in all cases, and the number of them for every document and the processing time is fairly satisfactory.  The information representation of the document, partly using the method, is discussed with relation to the construction of the document information retrieval system.\"\n",
    "\n",
    "tokenized_query = preprocess_text(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3c350e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 , arr2 = rank(tokenized_query,tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f50d980a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 419  483 1077  834  445  375  710 1390 1059 1179  460  785  211 1427\n",
      " 1003  872  149  251  979  522  724  595  249 1071  957  674 1363 1193\n",
      "  373  795 1190  500  205  441  663  158  227 1189  179  647  506  691\n",
      " 1206 1408 1247  812  420  564 1298  324   58 1426  166  610  369 1284\n",
      "   38  439  155 1251  449  970  703 1192   64  671 1151  730  489  174\n",
      "   48 1263 1432  847  679  528  178 1215  561  989  248  475  571  789\n",
      " 1011  134 1198  614  476  685 1108  570  863  888   25  377  533  164\n",
      "    5  603]\n",
      "[  70  575  315  353  552  482 1043  320  660 1225  340   27  642 1223\n",
      " 1164   15  387 1053 1390 1322  563  487   78  897 1090   45  654 1447\n",
      "  519   34  359  753  665  316  522  961 1072  797  114   41  655  293\n",
      " 1123  703 1391  486  449 1273  656  494  264  502 1173  958  648  837\n",
      "  529  724  477   61   75  175  699  861  362  783  819  441  640 1056\n",
      " 1410  380 1104  515  541  510  206  570  912  346 1141    1 1018  939\n",
      "  817 1019  149  701  643  794  328 1362  620  629  574  373 1161  727\n",
      "  804    5]\n"
     ]
    }
   ],
   "source": [
    "print(m)\n",
    "\n",
    "# new = []\n",
    "# for t in arr1:\n",
    "#     new.append(m[t])\n",
    "# print(new)\n",
    "print(arr1[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9ec16612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'moham', 'syria', 'wa', 'bear', '4/7/1998', 'love', 'celebr', 'birthday', '7-4-1998']\n"
     ]
    }
   ],
   "source": [
    "k = \"Hello My Name is Mohammed and I am from syria, I was born in 4/7/1998. I love to celebrate on my birthday in 7-4-1998\"\n",
    "new_k = preprocess_text(k)\n",
    "print(new_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1dc307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
